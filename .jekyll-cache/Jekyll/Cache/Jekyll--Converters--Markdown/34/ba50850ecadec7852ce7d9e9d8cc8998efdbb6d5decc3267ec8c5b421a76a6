I"Ä<!--end_excerpt-->
<h2 id="description">Description</h2>
<p>It‚Äôs a search algorithm to find the best choice from many options. It explores a graph by expanding the most promising node in a limited set. A beam search is usually used in situations where there isn‚Äôt a way to store the entire search tree in memory.</p>

<p><strong>where is it used in DL?</strong></p>

<p>sequence to sequence models like neural machine translation</p>

<p><strong>how is it used in DL?</strong></p>

<p>used to find the next best word, can be EOS</p>

<p><strong>For example:</strong></p>

<p>In NMT there are lots of possible combinations of words for a translation but we want to pick the best one, aka the one with the max probability distribution, and not one at random.</p>

<p><strong>Interesting things to note:</strong></p>

<p>When beam width = 1, then beam search is essentially a greedy algorithm</p>

<p>Beam search multiplies the log of the probabilities of the words to find the max probability - this leads to beam search favoring very short translations. This is kind of fixed by dividing it by the number of tokens.</p>

<h2 id="code">Code</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">beam_search_decoder</span><span class="p">(</span><span class="n">encoded_data</span><span class="p">,</span> <span class="n">beam_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">list</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">]]</span>
        <span class="c1"># walk over each step in sequence
</span>        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">encoded_data</span><span class="p">:</span>
            <span class="n">all_candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
            <span class="c1"># expand each current candidate
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)):</span>
                <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">)):</span>
                    <span class="c1"># why are we taking the log of the product of the probabilities
</span>                    <span class="c1"># instead of just the product of the probabilities?
</span>                    <span class="c1"># the probabilities are all numbers less than 1,
</span>                    <span class="c1"># multiplying a lot of numbers less than 1 will result in a very smol number
</span>                    <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">score</span> <span class="o">*</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">j</span><span class="p">])]</span>
                    <span class="n">all_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
            <span class="c1"># order all candidates by score
</span>            <span class="n">ordered</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">tup</span><span class="p">:</span> <span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="c1"># select beam_width best
</span>            <span class="n">sequence</span> <span class="o">=</span> <span class="n">ordered</span><span class="p">[:</span><span class="n">beam_width</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">sequence</span>
</code></pre></div></div>
<h2 id="math">Math</h2>
<p>How the algorithm works from Andrew Ng‚Äôs coursera course:</p>

<p><img src="/images/posts/beam_search1.png" alt="image" />
<img src="/images/posts/beam_search2.png" alt="image" />
<img src="/images/posts/beam_search3.png" alt="image" />
<img src="/images/posts/beam_search4.png" alt="image" /></p>

<h2 id="history">History</h2>
<p>The term ‚Äúbeam search‚Äù was coined by Raj Reddy, Carnegie Mellon University, 1977.</p>
:ET