<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>christina.kim</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 17 Apr 2020 03:38:54 -0700</pubDate>
    <lastBuildDate>Fri, 17 Apr 2020 03:38:54 -0700</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>

    
      <item>
        <title></title>
        <description>&lt;p&gt;A paper summary of &lt;a href=&quot;'https://arxiv.org/abs/1912.06208'&quot;&gt;Shaping representations through communication: community size effect in artificial learning systems
&lt;/a&gt;
&lt;!--end_excerpt--&gt;
This paper is motivated by co-adapation that occurs (information shared between them can become too specific) a) between a single encoder/speaker, and decoder/listener or b) when shared language arises within small groups. Since as long as the encoder, and decoder agree on the information, the learned information doesn’t need to be representative, abstractive, or systematic. This paper explores adding more encoders and decoders that are randomly paired up at each training step to encourage the encoders and decoders to learn a more general representation. They find that increasing the community size of encoders/decoders reduces idiosyncrasies in learned code and prevents co-adaption.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Apr 2020 03:38:54 -0700</pubDate>
        <link>http://localhost:4000/2020/04/17/2019-12-13-shaping-representations-through-communication-community/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/17/2019-12-13-shaping-representations-through-communication-community/</guid>
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;A paper summary of &lt;a href=&quot;'https://arxiv.org/abs/1908.10090'&quot;&gt;On NMT Search Errors and Model Errors: Cat Got Your Tongue?&lt;/a&gt;
&lt;!--end_excerpt--&gt;
Current NMT models may not be working correctly! When mostly given perfect conditions, aka an infinite search space, over half of the time, the models predict no translation (empty sequences).&lt;/p&gt;

&lt;p&gt;In this paper, the authors explore what NMT model’s predict as the best translation when the beam width is “infinite.” When the beam width is infinite, the model can consider all and any possibilities. Their technical contribution was coming up with a way to have models use infinite beam widths when searching. They found that in 51.8% of the cases, the model thought an empty sequence was the best translation! With beam search, longer sequences have lower probabilities, and it appears in many cases lower probabilities than a single EOS token.&lt;/p&gt;

&lt;p&gt;This paper is a startling contribution to NMT. It explores and exposes a very troubling bug of current NMT models. This paper is an exciting contribution since many NLP problems are modeled as a sequence to sequence problems, and many of the SOTA language models involve decoding with beam search. These are exciting implications and bring new questions for NLP research.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Apr 2020 03:38:54 -0700</pubDate>
        <link>http://localhost:4000/2020/04/17/2019-11-18-nmt-search-errors/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/17/2019-11-18-nmt-search-errors/</guid>
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;A paper summary of &lt;a href=&quot;'https://arxiv.org/abs/1911.03343'&quot;&gt;Negated LAMA: Birds cannot fly&lt;/a&gt;
&lt;!--end_excerpt--&gt;
When pre-trained language models, such as GPT-2, came out, I was curious about what they were learning and what applications could an LM, like GPT-2, have. What exactly was the model learning?&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/2019/11/14/lm-as-a-knowledge-base/&quot;&gt;Language Models as a Knowledge Base?&lt;/a&gt; the authors have a possible answer to that question. The idea was that LM is learning facts and understanding some things, and could you used them then as a knowledge base? The experiments they ran focused masking words in a cloze sentence to get the LM to predict what the answer would be. A cloze statement “is generated from a subject- relation-object triple from a knowledge base and from a template statement for the relation that contains variables X and Y for subject and object (e.g., “X was born in Y”).” For instance, “birds can MASK,” would return fly.&lt;/p&gt;

&lt;p&gt;The authors of Negated LAMA find that LM is not great with negation. When given “birds cannot MASK,” it returns fly as well. This suggests that LM are not actually understanding the text shoveled into it. This does show that LM’s are good at answering all questions regardless of accuracy, but the authors suggest that maybe it should not be giving an answer.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Apr 2020 03:38:54 -0700</pubDate>
        <link>http://localhost:4000/2020/04/17/2019-11-14-negated-lama/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/17/2019-11-14-negated-lama/</guid>
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;A paper summary of &lt;a href=&quot;'https://arxiv.org/pdf/1910.09260.pdf'&quot;&gt;Human-Like Decision Making: Document-level Aspect Sentiment
Classification via Hierarchical Reinforcement Learning&lt;/a&gt;
&lt;!--end_excerpt--&gt;&lt;/p&gt;

&lt;p&gt;Document-level Aspect Sentient Classification is a task to predict user’s sentiment polarities for different aspects of a product in a review. The authors propose using Hierarchical Reinforcement Learning to do the task as it’s more interpretable than other successful neural nets that perform well on DASC.&lt;/p&gt;

&lt;p&gt;HRL is pretty cool because it makes (more) intuitive sense as to how it works compared to other neural net architecture. Instead of regular RL, there are different tiers of policies. For DASC, the authors propose using a high-level policy to find the relevant clauses. A low-level policy to select sentiment-relevant words inside the selected clauses, they used a sentiment rating predictor that provides the reward signals to both the clause and word selection policies.&lt;/p&gt;

&lt;p&gt;The results of this method were comparable to state of the art methods of aspect sentiment classification. A cool thing that I appreciated the authors, including were ablation studies for HRL. They broke down how each component of the HRL architecture impacted the results.&lt;/p&gt;

&lt;p&gt;It was interesting to note that negated sentences didn’t work well with this method. I think negation is an interesting problem because if a neural net truly understands a sentence/clause, then I’d expect it to understand negations easily.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Apr 2020 03:38:54 -0700</pubDate>
        <link>http://localhost:4000/2020/04/17/2019-10-22-hrl/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/17/2019-10-22-hrl/</guid>
      </item>
    
      <item>
        <title>Grounded Language Learning</title>
        <description>&lt;!--end_excerpt--&gt;
&lt;h2 id=&quot;what-is-it&quot;&gt;What is it&lt;/h2&gt;
&lt;p&gt;Grounded Language Learning is the process of learning representations for words based on non-linguistic experience.
Why is it cool
Grounded language learning works to make use of language, multimodal information, and interactive environments. This research works toward achieving natural language understanding. Currently, natural language understanding is commonly tested by language models from text-only corpora. This approach with language models is based on the idea that the meaning of a word is based on only its relationship to other words. This is also called a distributional notion of semantics. SOTA language models are incredible for many different tasks and even come close to beating humans at natural language understanding benchmarks. However, there are critiques around NLU benchmarks, and grounded language learning argues that the words are not grounded in anything and, therefore, actually meaningless. With grounded language learning, the hope is to be able to create models that can understand and generalize well to their context.&lt;/p&gt;

&lt;p&gt;From VIGIL: Visually Grounded Interaction and Language&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“In neuroscience, recent progress in fMRI technology has enabled better understanding of the interaction between language, vision and other modalities suggesting that the brains share neural representations of concepts across vision and language.
In concurrent work, developmental cognitive scientists have argued that word acquisition in children is closely linked to them learning the underlying physical concepts in the real world and that they generalize surprisingly well at this from sparse evidence.”&lt;/p&gt;

  &lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;
  &lt;ul&gt;
    &lt;li&gt;Visual QA
&lt;img src=&quot;/images/posts/visual_qa.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Embodied QA
&lt;img src=&quot;/images/posts/embodied_qa.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Captioning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Visual-Audio Correspondence
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;&amp;quot;https://arxiv.org/abs/1705.08168&amp;quot;&quot;&gt;Look, Listen, Learn&lt;/a&gt; “what can be learned by training visual and audio networks simultaneously to predict whether visual information (a video frame) corresponds or not to audio information (a sound snippet)?”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;embodied agents performing interactive tasks
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://aihabitat.org/&quot;&gt;AI Habitat&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;StreetLearn
  &lt;a href=&quot;/images/posts/street_learn.png&quot;&gt;!image&lt;/a&gt;
  &lt;a href=&quot;/images/posts/street_learn2.png&quot;&gt;!image&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/House3D&quot;&gt;House3D&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;other games
    &lt;ul&gt;
      &lt;li&gt;Mechanical Turker Descent (MTD) that trains agents to execute natural language commands grounded in a fantasy text adventure game&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 04 Mar 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2020/03/04/grounded-language-learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/04/grounded-language-learning/</guid>
      </item>
    
      <item>
        <title>Beam Search</title>
        <description>&lt;!--end_excerpt--&gt;
&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;
&lt;p&gt;It’s a search algorithm to find the best choice from many options. It explores a graph by expanding the most promising node in a limited set. A beam search is usually used in situations where there isn’t a way to store the entire search tree in memory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;where is it used in DL?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;sequence to sequence models like neural machine translation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;how is it used in DL?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;used to find the next best word, can be EOS&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In NMT there are lots of possible combinations of words for a translation but we want to pick the best one, aka the one with the max probability distribution, and not one at random.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interesting things to note:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When beam width = 1, then beam search is essentially a greedy algorithm&lt;/p&gt;

&lt;p&gt;Beam search multiplies the log of the probabilities of the words to find the max probability - this leads to beam search favoring very short translations. This is kind of fixed by dividing it by the number of tokens.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;beam_search_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# walk over each step in sequence
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# expand each current candidate
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
                    &lt;span class=&quot;c1&quot;&gt;# why are we taking the log of the product of the probabilities
&lt;/span&gt;                    &lt;span class=&quot;c1&quot;&gt;# instead of just the product of the probabilities?
&lt;/span&gt;                    &lt;span class=&quot;c1&quot;&gt;# the probabilities are all numbers less than 1,
&lt;/span&gt;                    &lt;span class=&quot;c1&quot;&gt;# multiplying a lot of numbers less than 1 will result in a very smol number
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# order all candidates by score
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;ordered&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# select beam_width best
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ordered&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beam_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;math&quot;&gt;Math&lt;/h2&gt;
&lt;p&gt;How the algorithm works from Andrew Ng’s coursera course:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/beam_search1.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/images/posts/beam_search2.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/images/posts/beam_search3.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/images/posts/beam_search4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;p&gt;The term “beam search” was coined by Raj Reddy, Carnegie Mellon University, 1977.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Nov 2019 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2019/11/18/beam-search/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/18/beam-search/</guid>
      </item>
    
      <item>
        <title>Language Models as a Knowledge Base?</title>
        <description>&lt;p&gt;A paper summary of &lt;a href=&quot;'https://arxiv.org/pdf/1909.01066.pdf'&quot;&gt;Language Models as Knowledge Bases?&lt;/a&gt;
&lt;!--end_excerpt--&gt;
This paper explores using language models as a knowledge base. Structured knowledge bases are pretty restrictive and hard to manage. For that reason, there hasn’t been a lot of research interest in structured knowledge bases since the 80s since it appeared to be impractical for Q/A other fact-based NLP tasks.&lt;/p&gt;

&lt;p&gt;Language models make an attractive substitute for structured knowledge bases since they don’t run into the same issues structured knowledge bases had. They don’t require schema engineering, allow queries about an open class of relations, can easily extend to more data and require no human supervision to train.&lt;/p&gt;

&lt;p&gt;The authors propose LAMA (LAnguage Model Analysis), which is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models. They used this method to evaluate BERT. They found that BERT-large performs on par to a knowledge base from a text. Their experiments found that pretrained BERT-large was able to recall knowledge better than its other pretrained LM competitors and at a level remarkably competitive with non-neural and supervised alternatives.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Nov 2019 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2019/11/14/lm-as-a-knowledge-base/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/14/lm-as-a-knowledge-base/</guid>
      </item>
    
      <item>
        <title>Backpropagation</title>
        <description>&lt;!--end_excerpt--&gt;
&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;
&lt;p&gt;Backpropagation is short for backward propagation of errors. It’s used to compute gradients for our loss function in machine learning. It does this by computing the partial derivatives of each training example in regards to the weights and bias. Cost can be written as a function of the outputs from the neural network&lt;/p&gt;

&lt;p&gt;Another neat feature of backpropagation is that relates the neural network’s error and the weights and biases of the neural network’s last layer and it does this for the weights and biases of the last layer to the weights and biases of the second to last layer, and continues, using chain  rule.&lt;/p&gt;

&lt;p&gt;why is it important?&lt;/p&gt;

&lt;p&gt;there’s lots of ways you can compute the gradients for our loss function but most of them take too long for deep learning - requiring us to compute the cost in respects to each parameter each .&lt;/p&gt;

&lt;p&gt;Backpropagation is way faster - it allows us to simultaneously compute all the partial derivatives using one forward pass through the network, then one backward pass through the network.&lt;/p&gt;

&lt;h2 id=&quot;math&quot;&gt;Math&lt;/h2&gt;

&lt;p&gt;The actual algorithm:
&lt;img src=&quot;/images/posts/nielson_backprop.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Taken from Neural Networks and Deep Learning by Michael Nielson&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Nov 2019 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/2019/11/06/backpropagation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/06/backpropagation/</guid>
      </item>
    
      <item>
        <title>Skip Connections and Residual Blocks</title>
        <description>&lt;!--end_excerpt--&gt;
&lt;p&gt;&lt;img src=&quot;/images/posts/residual_block.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;
&lt;p&gt;This is a residual block, or skip connection. The residual is the difference between the predicted and target values. The words &lt;strong&gt;residual&lt;/strong&gt; and &lt;strong&gt;skip&lt;/strong&gt; are used interchangeably in many places. They can also be thought of as an identity block. If there is no residual then H(x) = x (aka the identity).&lt;/p&gt;

&lt;p&gt;A residual block is trying to learn the residual of the true distribution minus the input, while a normal layer tries to learn that true distribution&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;why does this work better?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Theoretically this shouldn’t matter. Neural networks are function approximations. The more layers you add, the better the approximation should be. However, this does not work in reality for many reasons such as exploding or vanishing gradients. By allowing the values to essentially pass through in a linear way, we can use previous gradients.&lt;/p&gt;

&lt;p&gt;In other words, a deeper net is not necessarily optimal because it might not learn the abstractions as the information learned in the earlier layers might disappear in the later layers.&lt;/p&gt;

&lt;p&gt;Less layers means a simpler model which is faster to train.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;code&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;


       &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
           &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BasicBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


           &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_planes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expansion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;math&quot;&gt;math&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;R(x) is the residual
H(x) is the output aka true distribution
x is the input in the layer

R(x) = Output — Input = H(x) — x

The residual plus the input is the true distribution
H(x) = R(x) + x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;history&quot;&gt;history&lt;/h2&gt;

&lt;p&gt;First introduced by microsoft in 2015.&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Oct 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2019/10/29/residual-blocks-and-skip-connections/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/29/residual-blocks-and-skip-connections/</guid>
      </item>
    
      <item>
        <title>Natural Language Understanding</title>
        <description>&lt;!--end_excerpt--&gt;
&lt;p&gt;Natural Language Understanding has been a focus in AI since the birth of AI as a field was defined. The 60s and 70s saw a burst of knowledge bases, ontologies, math word problem solvers. NLU is considered an AI hard problem (aka when we figure this out we have figured out general artificial intelligence).&lt;/p&gt;

&lt;p&gt;The tricky part of building a machine reading-comprehension system is that it requires a mostly-accurate (I say mostly accurate because unclear how specific human’s models of the world are) idea of the world, and the ability to generalize to new contexts/situations reasonably well. The ability to reason and make pretty good predictions in new cases given our prior knowledge and understanding of the world is what makes us great general purpose learners (and also helped us to exist till now probably).&lt;/p&gt;

&lt;p&gt;These are the current types of tasks and benchmarks that test for commonsense knowledge and reasoning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Textual Entailment&lt;/strong&gt;
Textual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively.
examples of tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RTE Challenges (Dagan, Glickman, &amp;amp; Magnini, 2005),&lt;/li&gt;
  &lt;li&gt;Story Cloze Test (Mostafazadeh, Chambers, He, Parikh, Batra, Vanderwende, Kohli, &amp;amp; Allen, 2016)&lt;/li&gt;
  &lt;li&gt;SWAG (Zellers, Bisk, Schwartz, &amp;amp; Choi, 2018)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Question Answering&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CommonsenseQA (Talmor, Herzig, Lourie  Berant, 2015)&lt;/li&gt;
  &lt;li&gt;Winograd Schema Challenge (Levesque, 2011)&lt;/li&gt;
  &lt;li&gt;GLUE (Wang, Singh, Michael, Hill, Levy, &amp;amp; Bowman, 2018)&lt;/li&gt;
  &lt;li&gt;Event2Mind (Rashkin, Sap, Allaway, Smith, &amp;amp; Choi, 2018b)&lt;/li&gt;
  &lt;li&gt;bAbI (Weston, Bordes, Chopra, Rush, van Merriënboer, Joulin, &amp;amp; Mikolov, 2015)&lt;/li&gt;
  &lt;li&gt;SuperGLUE(Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, Bowman, 2019)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 18 Jun 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2019/06/18/natural-language-understanding/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/06/18/natural-language-understanding/</guid>
      </item>
    

    
      
        
      
    
      
        
          <item>
            <title>About</title>
            <description>
</description>
            <link>http://localhost:4000/about.html</link>
          </item>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    

  </channel>
</rss>